{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Телеграм-бот для генерации стихотворений\n",
    "\n",
    "для запуска бота просто запустить все ячейки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-telegram-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#!pip install dialogflow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from telegram.ext  import Updater, CommandHandler, MessageHandler, Filters, CallbackContext\n",
    "import logging\n",
    "from telegram import Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Настройки\n",
    "updater = Updater(token='179767919:AAFB8nPqel_MhstnubHo0Qs1T9c_8yQ5Wo') # Токен API к Telegram\n",
    "dispatcher = updater.dispatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# предварительно нужно скачать модели\n",
    "#модель для генерации стихов Есенина\n",
    "#https://drive.google.com/drive/folders/1iCKCxtkIrjl-IAEswTUdgavqEMNOPylU?usp=sharing\n",
    "#модель для генерации стихов Лермонтова\n",
    "#https://drive.google.com/drive/folders/11cuy0YTItu_RdHi75X2Vh4F-Pm7pe54R?usp=sharing\n",
    "#\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def generate_lerm(prefix):\n",
    "    #загружаем модель\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"/Users/lilyarunich/Documents/datascience/vvedenie_v_obr_est_yazika/proj/lerm\") #путь до каталога с моделью\n",
    "    #токенизируем текст\n",
    "    tokens = tokenizer(prefix, return_tensors='pt')\n",
    "    tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "    size = tokens['input_ids'].shape[1]\n",
    "    output = model.generate(\n",
    "        **tokens,\n",
    "        #end_token=end_token_id,\n",
    "        do_sample=False,\n",
    "        max_length=size+100,\n",
    "        repetition_penalty=4.,\n",
    "        temperature=0.5,\n",
    "        num_beams=10,)\n",
    "    decoded = tokenizer.decode(output[0])\n",
    "    result = decoded[len(prefix):]\n",
    "    res = prefix + result\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def generate_esenin(prefix):\n",
    "    #загружаем модель\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"/Users/lilyarunich/Documents/datascience/vvedenie_v_obr_est_yazika/proj/esenin\") #путь до каталога с моделью\n",
    "    #токенизируем текст\n",
    "    tokens = tokenizer(prefix, return_tensors='pt')\n",
    "    tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "    size = tokens['input_ids'].shape[1]\n",
    "    output = model.generate(\n",
    "        **tokens,\n",
    "        #end_token=end_token_id,\n",
    "        do_sample=False,\n",
    "        max_length=size+100,\n",
    "        repetition_penalty=4.,\n",
    "        temperature=0.5,\n",
    "        num_beams=10,)\n",
    "    decoded = tokenizer.decode(output[0])\n",
    "    result = decoded[len(prefix):]\n",
    "    res = prefix + result\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few command handlers. These usually take the two arguments update and\n",
    "# context. Error handlers also receive the raised TelegramError object in error.\n",
    "def start(update: Update, context: CallbackContext):\n",
    "    update.message.reply_text('Hi!')\n",
    "\n",
    "def echo(update: Update, context: CallbackContext):\n",
    "    txt = update.message.text\n",
    "\n",
    "    if txt.lower() == 'привет':\n",
    "        update.message.reply_text('и тебе привет! ')\n",
    "    elif txt.lower() == 'хочу стих':\n",
    "        update.message.reply_text('выбери автора: 1 - Лермонтов, 2 - Есенин')\n",
    "        update.message.reply_text('введи запрос в формате: \"_число_ _начало стихотворения_\"')\n",
    "    elif txt.split()[0] == '1':\n",
    "        update.message.reply_text('ожидайте - началась генерация стихотворения Лермонтова ' + txt[2:])\n",
    "        update.message.reply_text(generate_lerm(txt[2:]))\n",
    "    elif txt.split()[0] == '2':\n",
    "        update.message.reply_text('ожидайте - началась генерация стихотворения Есенина ' + txt[2:])\n",
    "        update.message.reply_text(generate_esenin(txt[2:]))\n",
    "    else:\n",
    "        update.message.reply_text('если хотите начать генерацию стихотворения - введите \"хочу стих\" и следуйте инструкциям ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 14:36:20,097 - apscheduler.scheduler - INFO - Scheduler started\n",
      "Distant resource does not have an ETag, we won't be able to reliably ensure reproducibility.\n",
      "2021-08-31 14:37:20,737 - telegram.ext.dispatcher - ERROR - No error handlers are registered, logging exception.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 512, in get_config_dict\n",
      "    resolved_config_file = cached_path(\n",
      "  File \"/usr/local/lib/python3.8/site-packages/transformers/file_utils.py\", line 1370, in cached_path\n",
      "    output_path = get_from_cache(\n",
      "  File \"/usr/local/lib/python3.8/site-packages/transformers/file_utils.py\", line 1547, in get_from_cache\n",
      "    raise OSError(\n",
      "OSError: Distant resource does not have an ETag, we won't be able to reliably ensure reproducibility.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/site-packages/telegram/ext/dispatcher.py\", line 555, in process_update\n",
      "    handler.handle_update(update, self, check, context)\n",
      "  File \"/usr/local/lib/python3.8/site-packages/telegram/ext/handler.py\", line 198, in handle_update\n",
      "    return self.callback(update, context)\n",
      "  File \"<ipython-input-8-4364d8ef104a>\", line 16, in echo\n",
      "    update.message.reply_text(generate_lerm(txt[2:]))\n",
      "  File \"<ipython-input-5-6e8657d4f44a>\", line 5, in generate_lerm\n",
      "    model = AutoModelForCausalLM.from_pretrained(\"https://drive.google.com/drive/folders/11cuy0YTItu_RdHi75X2Vh4F-Pm7pe54R?usp=sharing\")\n",
      "  File \"/usr/local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 378, in from_pretrained\n",
      "    config, kwargs = AutoConfig.from_pretrained(\n",
      "  File \"/usr/local/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\", line 450, in from_pretrained\n",
      "    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 532, in get_config_dict\n",
      "    raise EnvironmentError(msg)\n",
      "OSError: Can't load config for 'https://drive.google.com/drive/folders/11cuy0YTItu_RdHi75X2Vh4F-Pm7pe54R?usp=sharing'. Make sure that:\n",
      "\n",
      "- 'https://drive.google.com/drive/folders/11cuy0YTItu_RdHi75X2Vh4F-Pm7pe54R?usp=sharing' is a correct model identifier listed on 'https://huggingface.co/models'\n",
      "\n",
      "- or 'https://drive.google.com/drive/folders/11cuy0YTItu_RdHi75X2Vh4F-Pm7pe54R?usp=sharing' is the correct path to a directory containing a config.json file\n",
      "\n",
      "\n",
      "2021-08-31 14:37:39,718 - telegram.ext.updater - INFO - Received signal 2 (SIGINT), stopping...\n",
      "2021-08-31 14:37:39,719 - apscheduler.scheduler - INFO - Scheduler has been shut down\n"
     ]
    }
   ],
   "source": [
    "updater = Updater(\"1797679219:AAFB8nPqDel_MhstnubHo0Qs1T9c_8yQ5Wo\", use_context=True)\n",
    "dispatcher = updater.dispatcher\n",
    "\n",
    "# on different commands - answer in Telegram\n",
    "dispatcher.add_handler(CommandHandler(\"start\", start))\n",
    "dispatcher.add_handler(MessageHandler(Filters.text & ~Filters.command, echo))\n",
    "\n",
    "# Start the Bot\n",
    "updater.start_polling()\n",
    "updater.idle()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}